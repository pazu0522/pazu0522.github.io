---
title:  "수리통계학(Stats 111) : Chapter 04. Unbiasedness, Consistency and Limiting Distribution (2)"
excerpt: "부산대학교 김충락 교수의 수리통계학 강의 정리"

categories:
  - 통계학
  - 수리통계학
tags:
  - 통계량
  - 큰 수의 법칙
  - 확률수렴
  - 일치성
use_math: true
comments: true
last_modified_at: 2021-07-06T23:30:00
header:
  image: /assets/images/texture.jpg
---

<img src="/assets/images/stats_111/lesson_0402.jpg" width="75%" height="75%" title="dart" alt="dart"/> 

# 1. 확률수렴

- [**확률수렴의 정의**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/stats111_0402/#%ED%99%95%EB%A5%A0%EC%88%98%EB%A0%B4%EC%9D%98-%EC%A0%95%EC%9D%98)
- [**약한 큰 수의 법칙**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/stats111_0402/#%EC%95%BD%ED%95%9C-%ED%81%B0-%EC%88%98%EC%9D%98-%EB%B2%95%EC%B9%99)
- [**확률수렴 관련 정리**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/stats111_0402/#%ED%99%95%EB%A5%A0%EC%88%98%EB%A0%B4-%EA%B4%80%EB%A0%A8-%EC%A0%95%EB%A6%AC)
- [**일치추정량**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/stats111_0402/#%EC%9D%BC%EC%B9%98%EC%B6%94%EC%A0%95%EB%9F%89)


## 확률수렴의 정의

확률변수의 **수열** $\\{X_{n}\\}$와 확률변수 $X$가 존재한다. 여기서 확률변수의 수열이란 확률변수 $X$가 따르는 확률분포로부터의 **Random Sample**로 만들어 낸 **통계량**이다. 예를 들어 $X_{i} \overset {i.i.d} \sim f_{X}(x)$일 때, 확률변수의 수열 $\\{Y_{n}\\}$을 $\frac{\sum X_{i}}{n}$ 즉, 표본평균으로 정의한다면 $Y_{1} = X_{1}, \; Y_{2} = \frac{X_{1}+X_{2}}{2}, \; Y_{3} = \frac{X_{1}+X_{2}+X_{3}}{3}, \cdots$와 같이 전개될 것이다. 따라서 표본평균은 확률변수의 수열이다.

모든 **양수** $\varepsilon$에 대하여 $n \rightarrow \infty$에 따라 $P( \mid X_{n}-X \mid > \varepsilon) \rightarrow 0$ 또는 $P( \mid X_{n}-X \mid \leq \varepsilon) \rightarrow 1$이면 확률변수의 수열 $X_{n}$이 확률변수 $X$로 **확률수렴**한다고 한다. 확률수렴의 표기는 $X_{n} \overset{P} \rightarrow X$와 같이 한다. 확률수렴의 성립 조건을 수식으로 다시 한 번 정리하면 다음과 같다.

\begin{align}
P( \mid X_{n}-X \mid > \varepsilon) \rightarrow 0 \;\; \text{for} \;\; ^{\forall} \varepsilon > 0, \;\; \text{as} \;\; n  \to \infty \nonumber
\end{align} 

만약 확률변수 $X$가 항상 어떤 **상수** $c$의 값을 가지는 확률변수라면 이 확률변수는 결국 상수와 같다. 따라서 확률수렴은 상수 $c$로도 가능하다. 즉, $X_{n} \overset{P} \rightarrow c$도 확률수렴이다.


## 약한 큰 수의 법칙

$X_{i} \overset {i.i.d} \sim f_{X}(x)$인 Random Sample에 대하여 $E(X_{i}) = \mu$이고 $Var(X_{i}) = \sigma^{2} < \infty$이면, **표본평균** $\bar{X}$은 **모평균** $\mu$로 **확률수렴**한다. 즉, $\bar{X} \overset{P} \to \mu$이다. 이를 **약한 큰 수의 법칙(Weak Law of Large Numbers; WLLN)**이라 한다. WLLN은 [체비셰프 부등식](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%ED%99%95%EB%A5%A0%EB%A1%A0/stats110_28/#%EC%B2%B4%EB%B9%84%EC%85%B0%ED%94%84-%EB%B6%80%EB%93%B1%EC%8B%9D)으로 증명할 수 있다.

체비셰프 부등식에 의해, $E(X) = \mu$이고 $Var(X) = \sigma^{2} < \infty$이면 $P( \mid X-E(X) \mid > c) \leq \frac{Var(X)}{c^{2}}$가 성립한다.

표본평균 $\bar{X}$는 확률변수의 수열이고, 확률변수의 수열은 Random Sample의 함수이다. 따라서 표본평균 $\bar{X}$ 역시 하나의 확률변수이다. 이제 위 체비셰프 부등식에서 $X$를 $\bar{X}$로, $c$를 $\varepsilon$으로 변경하면 $P( \mid \bar{X}-E(\bar{X}) \mid > \varepsilon) \leq \frac{Var(\bar{X})}{\varepsilon^{2}}$가 성립한다. 그런데 $E(\bar{X}) = E(X_{i}) = \mu$이고, $Var(\bar{X}) = \frac{Var(X_{i})}{n} = \frac{\sigma^{2}}{n}$이므로 다음과 같은 전개가 가능하다.

\begin{align}
P( \mid \bar{X}-E(\bar{X}) \mid > \varepsilon) \leq \frac{Var(\bar{X})}{\varepsilon^{2}} = \frac{Var(X_{i})}{n} \cdot \frac{1}{\varepsilon^{2}} \to 0, \quad \text{as} \;\; n \to \infty \nonumber
\end{align}

\begin{align}
\therefore \bar{X} \overset{P} \to E(\bar{X}) = E(X_{i}) = \mu \nonumber
\end{align}

만약 $Var(X_{i})$가 **유한**하지 않다면 $\frac{Var(X_{i})}{n}$이 수렴하지 않으므로 WLLN이 성립하지 않는다.

이제 함수 $Y = f(X)$에 대하여 $\bar{Y}$ 역시 WLLN을 만족하는지 확인한다. $\bar{Y} = \frac{1}{n} \sum f(X_{i})$이므로 $E(\bar{Y}) = E(f(X_{i}))$이고 $Var(\bar{Y}) = \frac{Var(f(X_{i}))}{n} = \frac{E \[f(X_{i}^{2})\] - \[E(f(X_{i}))\]^{2}}{n}$이다. $E(f(X_{i}))$가 존재하고, $E\[f(X_{i}^{2})\]$가 **유한**한 값을 가진다면 같은 전개로 $\bar{Y} \overset {P} \to E(Y_{i}) = E(f(X_{i}))$가 성립할 것이다.

따라서 $f(X_{i})$의 기댓값이 존재하고 분산이 유한하다면, $\bar{X^{k}} \overset{P} \to E(X_{i}^k)$ 등이 성립한다.


## 확률수렴 관련 정리

> $X_{n} \overset{P}\to X, \;\; Y_{n} \overset{P}\to Y \;\Rightarrow \; X_{n} + Y_{n} \overset{P}\to X+Y$

위 성질의 증명은 확률수렴의 정의대로, $n \to \infty$에 따라 $P( \mid X_{n} + Y_{n} \mid - (X+Y) > \varepsilon) \to 0$를 보이면 된다.

증명 과정에서 **삼각부등식**을 사용하는데 삼각부등식과 그 증명은 다음과 같다.

\begin{align}
\-\mid x \mid \; \leq x \leq \; \mid x \mid, \quad - \mid y \mid \; \leq  y \leq \; \mid y \mid \nonumber
\end{align}

\begin{align}
x+y \leq \; \mid x \mid + \mid y \mid \nonumber
\end{align}

\begin{align}
\therefore \; \mid x+y \mid \; \leq \; \mid x \mid + \mid y \mid \nonumber
\end{align}

**삼각부등식**에 의해 $\mid (X_{n} + Y_{n}) - (X+Y) \mid \;\; = \;\; \mid (X_{n} - X) + (Y_{n} - Y ) \mid  \;\; \leq \;\; \mid X_{n}-X \mid  + \mid Y_{n}-Y \mid$가 성립하므로 다음 부등식도 역시 성립한다.

\begin{equation\*}
\begin{aligned}
P( \mid X_{n} + Y_{n} \mid - (X+Y) > \varepsilon) 
&\leq P( \mid X_{n}-X \mid  + \mid Y_{n}-Y \mid \; > \varepsilon) \\\\\\ \\\\\\ 
&\leq P(\mid X_{n}-X \mid \; > \varepsilon/2 \;\; \cup \; \mid Y_{n}-Y \mid \; > \varepsilon/2 ) \\\\\\ \\\\\\
&\leq P(\mid X_{n}-X \mid \; > \varepsilon/2 ) + P( \mid Y_{n}-Y \mid \; > \varepsilon/2 )
\end{aligned}
\end{equation\*}


첫 번째 부등식에서 양변은 각 수식의 계산 결과가 임의의 $\varepsilon$보다 클 확률을 나타낸다. 수식의 계산 결과가 크면 클수록 그 값이 임의의 $\varepsilon$보다 클 확률이 높아진다. 본인이 주사위를 굴려서 나온 값이 상대방이 주사위를 굴려서 나온 값보다 크면 승리하는 게임이 있을 때, 상대방의 값은 $\varepsilon$에 해당한다. 당연히 본인의 값이 크면 클수록 게임에서 승리할 확률이 높아진다. 같은 원리에서 우변의 확률은 좌변의 확률보다 크거나 같다.

두 번째 부등식은 좌변보다 우변이 더 큰 **집합**이기 때문에 성립한다. 예를 들어 $A+B > c$가 성립하면 $A > c/2$ 또는 $B > c/2$이다. 그러나 **역**은 성립하지 않는다. 같은 원리로 좌변의 표본공간보다 우변의 표본공간이 크기 때문에 확률 또한 우변이 좌변보다 크거나 같다.

세 번째 부등식은 **합사건** 공식에 의해 성립한다. 합사건의 확률은 개별 사건 확률의 합에 **교사건**의 확률을 뺀 값이다. 그러나 우변은 교사건의 확률을 차감하지 않은 상태이므로 우변의 확률이 좌변의 확률보다 크거나 같다.

초기 조건에서 $X_{n} \overset{P}\to X$이고, $Y_{n} \overset{P}\to Y$였으므로 확률수렴의 성질에 의해 $n \to \infty$에 따라 $P(\mid X_{n}-X \mid \; > \varepsilon/2 )$와 $P(\mid Y_{n}-Y \mid \; > \varepsilon/2 )$ 모두 $0$으로 수렴한다. 결국 $P( \mid X_{n} + Y_{n} \mid - (X+Y) > \varepsilon)$ 또한 $0$으로 수렴하므로 증명이 완료되었다.

> $X_{n} \overset{P}\to X \; \Rightarrow \; cX_{n} \overset{P}\to cX$

이 성질은 확률변수 앞에 곱해진 상수 $c$를 절댓값 밖으로 빼내면 간단하게 증명 가능하다.

\begin{align}
P( \mid c \cdot X_{n} - c \cdot X \mid \; > \varepsilon) = P( \mid c \mid \cdot \mid X_{n}-X \mid \; > \varepsilon) = P \left( \mid X_{n} - X  \mid \; > \frac{\varepsilon}{\mid c \mid} \right) \nonumber 
\end{align}

\begin{align}
\text{as}\;\; n \to \infty, \quad P \left( \mid X_{n} - X  \mid \; > \frac{\varepsilon}{\mid c \mid} \right) \to 0 \nonumber 
\end{align}

> $X_{n} \overset {P} \rightarrow a$이고, 함수 $g$가 $a$에서 **연속**이면 $g(X_{n}) \overset {P} \rightarrow g(a)$  
> $X_{n} \overset {P} \rightarrow X$이고, 함수 $g$가 **연속**이면 $g(X_{n}) \overset {P} \rightarrow X$

**Continuous Mapping Theorem**이라 불리는 정리이다. 두 번째 정리는 증명 과정이 학부 수준을 넘어서기 때문에 여기서는 첫 번째 정리만 증명한다.

함수 $g$가 $a$에서 연속이라면 임의의 **양수** $\varepsilon$에 대하여, $\mid x-a \mid \; < \delta$를 만족할 때 $\mid g(x)-g(a) \mid \; < \varepsilon$를 만족하도록 만드는 **양수** $\delta$가 항상 존재한다.

즉, 함수 $g$가 $a$에서 연속일 때 $\mid x-a \mid \; < \delta$이면 $\mid g(x)-g(a) \mid \; < \varepsilon$이다. 이 명제가 참이므로 대우 명제 $\mid g(x)-g(a) \mid \; \geq \varepsilon$이면 $\mid x-a \mid \; \geq \delta$도 참이다.

$a \rightarrow b$일 때 $A \subset  B$이고 $P(A) \leq P(B)$이므로, 같은 원리로 $P(\mid g(x)-g(a) \mid \; \geq \varepsilon) \leq P(\mid x-a \mid \; \geq \delta)$이다.

조건에서 $X_{n} \overset {P} \rightarrow a$을 만족한다는 것은 $n \to \infty$에 따라 $P(\mid x-a \mid \; \leq \delta) \to 0$을 만족한다는 것이다. 따라서 $P(\mid g(x)-g(a) \mid \; \geq \varepsilon) \to 0$ 또한 성립하므로 증명이 완료되었다.

확률수렴의 정리들을 이용하여 $X_{n} \overset{P} \to X$이고 $Y_{n} \overset{P} \to Y$이면 $X_{n}Y_{n} \overset{P} \to XY$가 성립함을 보일 수 있다.

\begin{align}
X_{n}Y_{n} = \frac{X_{n}^{2}}{2} + \frac{Y_{n}^{2}}{2} - \frac{(X_{n}-Y_{n})^{2}}{2} \overset{P} \to \frac{X^{2}}{2} + \frac{Y^{2}}{2} - \frac{(X-Y)^{2}}{2} \nonumber 
\end{align}


## 일치추정량

어떤 통계량 $T = \upsilon(X_{1}, \cdots, X_{n})$에 대하여 $n \to \infty$에 따라 $T \overset{P} \to \theta$가 성립한다면 $T$는 $\theta$에 대한 **일치추정량(Consistent Estimator)**이다. 즉, 일치추정량은 어떤 **모수**로 **확률수렴**하는 통계량이다.

예를 들어, $X_{i} \overset {i.i.d} \sim f_{X}(x)$에 대하여 $E(X_{i}) = \mu$이고 $Var(X_{i}) = \sigma^{2} < \infty$라고 가정한다. 이 때 **표본분산** $s^{2}$는 **모분산** $\sigma^{2}$에 대한 일치추정량이다. 증명을 위해서는 일치추정량의 정의에 따라, 표본분산이 모분산으로 확률수렴하는지 여부를 확인하면 된다.

\begin{equation\*}
\begin{aligned}
s^{2}
&= \frac{1}{n-1} \sum_{i}^{n}(X_{i} - \bar{X})^{2} \\\\\\ \\\\\\
&= \frac{1}{n-1} \sum_{i}^{n}(X_{i}^{2} -2\bar{X}X_{i} + \bar{X}^{2})  \\\\\\ \\\\\\
&= \frac{1}{n-1} \left(\sum_{i}^{n}X_{i}^{2} - n\bar{X}^{2} \right)  \\\\\\ \\\\\\
&= \frac{n}{n-1} \left(\frac{1}{n}\sum_{i}^{n}X_{i}^{2} - \bar{X}^{2} \right)  \\\\\\ \\\\\\
\end{aligned}
\end{equation\*}

**WLLN**과 **Continuous Mapping Theorem**에 의하여 $\frac{1}{n}\sum_{i}^{n}X_{i}^{2} \overset{P} \to E(X_{i}^{2}) = \mu^{2} + \sigma^{2}$이고, $\bar{X}^{2} \overset{P} \to \mu^{2}$이다. 또한 $n \to \infty$에 따라 $\frac{n}{n-1} \to 1$이다. 따라서 다음이 성립하게 된다.

\begin{align}
\frac{n}{n-1} \left(\frac{1}{n}\sum_{i}^{n}X_{i}^{2} - \bar{X}^{2} \right) \overset{P} \to 1 \cdot (\mu^{2} + \sigma^{2} - \mu^{2}) = \sigma^{2} \nonumber 
\end{align}

\begin{align}
\therefore s^{2} \to \sigma^{2} \nonumber 
\end{align}