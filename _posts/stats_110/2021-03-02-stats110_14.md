---
title:  "확률론 기초(Stats 110) : Lesson 14. Law of the Unconscious Statistician"
excerpt: "하버드 대학교 조셉 K. 블리츠타인 교수의 Stats 110 14강 정리"

categories:
  - 통계학
  - 확률론
tags:
  - 정규분포
  - LOTUS
  - 분산
use_math: true
comments: true
last_modified_at: 2021-03-02T23:30:00
header:
  image: /assets/images/texture.jpg
---

<img src="/assets/images/stats_110/lesson_14.jpg" width="70%" height="60%" title="lotus" alt="lotus"/> 

# 1. 정규분포

- [**표준화**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%ED%99%95%EB%A5%A0%EB%A1%A0/stats110_14/#%ED%91%9C%EC%A4%80%ED%99%94)
- [**정규분포의 성질**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%ED%99%95%EB%A5%A0%EB%A1%A0/stats110_14/#%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC%EC%9D%98-%EC%84%B1%EC%A7%88)
- [**68, 95, 99.7의 법칙**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%ED%99%95%EB%A5%A0%EB%A1%A0/stats110_14/#68-95-997%EC%9D%98-%EB%B2%95%EC%B9%99)


## 표준화

$Z \sim N(0, 1)$에 대하여 확률변수 $X = \mu + \sigma Z$이면 $X$의 **기댓값**과 **분산**은 다음과 같이 계산된다.

\begin{align}
E(X) = E(\mu + \sigma Z) = \mu + \sigma(E(Z)) = \mu + \sigma \cdot 0 = \mu \nonumber
\end{align}

\begin{align}
Var(X) = Var(\mu + \sigma Z) = \sigma^{2} Var(Z) = \sigma^{2} \cdot 1 = \sigma^{2} \nonumber
\end{align}

\begin{align}
\because Var(X+c) = Var(X), \quad Var(cX) = c^{2} Var(X) \nonumber
\end{align}

$Var(cX) = c^{2} Var(X)$인 이유는 $Var(cX) = E(c^{2}X^{2}) - {\\{cE(X)\\}}^{2} = c^{2}(E(X^{2}) - {\\{E(X)\\}}^{2})$이기 때문이다.

또한 $X \sim N(\mu, \sigma^{2}), \;\; \sigma > 0$가 성립한다. 이 때 $\mu$는 **평균**으로 정규분포의 **위치**를 결정하고 $\sigma$는 **표준편차**로 **분포의 전체적인 모양**을 조절한다. 

증명은 $X$의 CDF를 미분하여 얻은 PDF가 **정규분포**의 PDF와 일치함을 보이면 된다.

\begin{align}
P(X \leq x) = P(\mu + \sigma Z \leq x) = P(Z \leq \frac{x-\mu}{\sigma}) = \Phi (\frac{x-\mu}{\sigma}) \nonumber
\end{align}

이제 미분의 **연쇄법칙**을 이용하여 $\Phi (\frac{x-\mu}{\sigma})$를 미분한다. 즉 $y = \Phi(z)$이고, $z = \frac{x-\mu}{\sigma}$이다. $\Phi(z)$를 미분하면 **표준정규분포**의 PDF와 같다.

\begin{align}
\left( \Phi (\frac{x-\mu}{\sigma}) \right)' = f(\frac{x-\mu}{\sigma}) \cdot \frac{1}{\sigma} = \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2} (\frac{x-\mu}{\sigma})^{2}} \cdot \frac{1}{\sigma} = \frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{1}{2} (\frac{x-\mu}{\sigma})^{2}}\nonumber
\end{align}

위에서 얻은 정규분포의 PDF를 $z = \frac{x-\mu}{\sigma}$로 치환하면 $dx = \sigma dz$가 되어 표준정규분포의 PDF와 같아지므로 적분값이 $1$이 된다는 사실을 알 수 있다.

이처럼 $X = \mu + \sigma Z$일 때, $Z = \frac{X-\mu}{\sigma}$로의 변환을 **표준화**라 한다.


## 정규분포의 성질

- $X \sim N(\mu, \sigma^{2})$이고 $Y = aX+b, \quad a \neq 0$이면, $Y \sim N(a\mu+b, a^{2}\sigma^{2})$
- $X \sim N(\mu, \sigma^{2})$이면 $-X \sim N(-\mu, \sigma^{2})$

위 성질의 증명 역시 $Y$의 CDF에서 시작한다.

\begin{align}
P(Y \leq y) = P(X \leq \frac{y-b}{a}) = F\left(\frac{y-b}{a}\right) \nonumber
\end{align}

\begin{align}
F\left(\frac{y-b}{a}\right)' = f\left(\frac{y-b}{a} \right) \cdot \frac{1}{a} = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{\frac{y-b}{a}-\mu}{\sigma}\right)^{2}} \cdot \frac{1}{a} = \frac{1}{\sqrt{2\pi} \cdot a\sigma}e^{-\frac{1}{2}\left(\frac{y-(a\mu+b)}{a\sigma}\right)^{2}} \nonumber
\end{align}

\begin{align}
\therefore Y \sim N(a\mu+b, a^{2}\sigma^{2}) \nonumber
\end{align}

두 번째 성질은 첫 번째 성질의 응용으로 $Y=aX+b$에서 $a=-1$이고 $b=0$인 경우에 해당한다.

- 서로 **독립**인 $X_{j} \sim N(\mu_{j}, \sigma^{2}\_{j})$에 대하여 $(X_{1}+X_{2}) \sim N(\mu_{1}+\mu_{2}, \sigma^{2}\_{1}+\sigma^{2}_{2})$
- 서로 **독립**인 $X_{j} \sim N(\mu_{j}, \sigma^{2}\_{j})$에 대하여 $(X_{1}-X_{2}) \sim N(\mu_{1}-\mu_{2}, \sigma^{2}\_{1}+\sigma^{2}_{2})$

정규분포를 따르고 서로 **독립**인 확률변수의 **합 또는 차**로 정의된 확률변수 역시 정규분포를 따른다. 위 성질은 확률변수의 **합성곱**을 통해서 [증명](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables)할 수 있다. 확률변수의 합성곱을 알고 있다면 증명의 원리 자체는 어렵지 않으나 실제 계산 과정이 굉장히 복잡하다. 다행히 위키피디아에 계산 과정이 매우 자세하게 설명되어 있다.


## 68, 95, 99.7의 법칙

$X \sim N(\mu, \sigma^{2})$일 때 $P(\mid X-\mu \mid \leq \sigma)$ 즉, 확률변수 $X$의 값과 **평균**과의 차이가 $1$ **표준편차** 이내일 확률은 약 $0.68$이다. 이와 마찬가지로 차이가 $2$ **표준편차** 이내일 확률, $3$ **표준편차** 이내일 확률은 각각 약 $0.95$와 $0.997$이다.

- $P(\mid X-\mu \mid \leq \sigma) \fallingdotseq 0.68$
- $P(\mid X-\mu \mid \leq 2\sigma) \fallingdotseq 0.95$
- $P(\mid X-\mu \mid \leq 3\sigma) \fallingdotseq 0.997$

* * * 
<br>

# 2. 합성함수의 기댓값

- [**LOTUS**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%ED%99%95%EB%A5%A0%EB%A1%A0/stats110_14/#lotus)
- [**확률분포의 분산**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%ED%99%95%EB%A5%A0%EB%A1%A0/stats110_14/#%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC%EC%9D%98-%EB%B6%84%EC%82%B0)


## LOTUS

무의식적인 통계학자의 법칙(**LOTUS**; Law of the Unconscious Statistician)을 통해 **합성함수의 기댓값**을 손쉽게 계산할 수 있다. 예를 들어, $f(x) = x^{2}$라는 함수에 대하여 $f(X) = X^{2}$의 기댓값을 계산하기 위해 원래대로라면 $X^{2}$의 PMF 또는 PDF를 계산해야 한다. 하지만 LOTUS를 이용하면 다음과 같이 기댓값을 계산할 수 있다.

- **이산확률변수** : $E(g(X)) = \sum_{x}g(x)P(X=x)$
- **연속확률변수** : $E(g(X)) = \int_{-\infty}^{\infty}g(x)f(x)dx$

연속확률변수에서 $f(x)$는 확률변수 $X$의 PDF를 의미한다. LOTUS의 증명은 [조약돌 논리](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%ED%99%95%EB%A5%A0%EB%A1%A0/stats110_04/#%EC%A7%81%EA%B4%80%EC%A0%81-%ED%95%B4%EC%84%9D)를 다시 한 번 이용한다. $g(X)$의 기댓값은 표본 공간 내 각각의 조약돌 $s$에 대해 $g(X_{(s)})$를 계산한 후, 계산된 값을 조약돌 각각의 질량(확률)으로 가중평균한 값과 같다. 이를 수식으로 나타내면 다음과 같다.

\begin{align}
E(g(X)) = \sum_{s \in S}g(X_{(s)})P(\\{s\\}) \nonumber
\end{align}

위 식은 표본공간 내 원소 즉, 조약돌을 모두 따로따로 취급하여 기댓값을 계산하는 방법이다. 하지만 확률변수에 의해 같은 값으로 매핑되는 조약돌들을 한데 모은 다음, 같은 그룹으로 묶인 조약돌 질량의 합으로 가중평균을 해도 결과는 마찬가지이다. 따라서 다음이 성립한다.

\begin{align}
\sum_{s \in S}g(X_{(s)})P(\\{s\\}) = \sum_{x}g(x) \left(\sum_{s:X_{(s)}=x}P(\\{s\\}) \right) = \sum_{x}g(x)P(X=x) \nonumber
\end{align}

위 식에서 $\sum_{s:X_{(s)}=x}P(\\{s\\})$가 의미하는 것은 확률변수에 의해 같은 값으로 매핑되는 조약돌들의 질량 합계인데 이것은 결국 $P(X=x)$와 같다.

LOTUS에 의해 $X^{2}$의 PMF를 계산하지 않아도 $E(X^{2}) = \sum_{x}x^{2}P(X=x)$임을 알 수 있다.


## 확률분포의 분산

**LOTUS**를 이용하여 $E(X^{2})$를 계산할 수 있게 되었으므로 이산확률분포의 **분산** 역시 계산할 수 있다.

> 포아송분포

**포아송분포**의 PMF는 $\frac{e^{-\lambda}\lambda^{k}}{k!}$이다. $X \sim Pois(\lambda)$일 때, $E(X^{2})$는 **LOTUS**에 의해 다음과 같다.

\begin{align}
E(X^{2}) = \sum_{k=0}^{\infty}k^{2} \cdot \frac{e^{-\lambda}\lambda^{k}}{k!} = e^{-\lambda} \cdot \lambda \cdot e^{\lambda}(1+\lambda) = \lambda^{2} + \lambda \nonumber
\end{align}

위 식에서 시그마를 풀어내는 과정은 다음과 같다.

\begin{align}
\sum_{k=0}^{\infty}\frac{\lambda^{k}}{k!} = e^{\lambda} \nonumber
\end{align}

**테일러 급수**에 의해 위와 같이 쓸 수 있으며 양변을 $\lambda$에 대하여 **미분**한다. 그런데 미분을 하면 $\lambda$의 차수가 하나 감소하므로 다시 양변에 $\lambda$를 곱하여 차수를 원래대로 맞추어준다.

\begin{align}
\sum_{k=0}^{\infty} k \cdot \frac{\lambda^{k-1}}{k!} = e^{\lambda} \nonumber
\end{align}

\begin{align}
\lambda \cdot \sum_{k=0}^{\infty} \frac{k \cdot \lambda^{k-1}}{k!} = \sum_{k=0} \frac{k \cdot \lambda^{k}}{k!} = \lambda \cdot e^{\lambda} \nonumber
\end{align}

이제 한 번 더 $\lambda$에 대해 **미분** 후 차수를 재조정하면 된다.

\begin{align}
\sum_{k=0}^{\infty} \frac{k^{2} \cdot \lambda^{k-1}}{k!} = e^{\lambda} + \lambda \cdot e^{\lambda} \nonumber
\end{align}

\begin{align}
\lambda \cdot \sum_{k=0}^{\infty} \frac{k^{2} \cdot \lambda^{k-1}}{k!} = \sum_{k=0}^{\infty} \frac{k^{2} \cdot \lambda^{k}}{k!} = \lambda \cdot e^{\lambda}(1+\lambda) \nonumber
\end{align}

\begin{align}
\therefore \sum_{k=0}^{\infty}k^{2} \cdot \frac{e^{-\lambda}\lambda^{k}}{k!} = e^{-\lambda} \cdot \lambda \cdot e^{\lambda}(1+\lambda) = \lambda^{2} + \lambda\nonumber
\end{align}

포아송분포를 따르는 확률변수의 **기댓값**이 $\lambda$와 같다는 사실을 이미 알고 있으므로, **분산** 계산을 위한 모든 준비를 마쳤다.

\begin{align}
Var(X) = E(X^{2}) - \\{E(X)\\}^{2} = (\lambda + \lambda^{2}) - \lambda^{2} = \lambda \nonumber
\end{align}

포아송분포를 따르는 확률변수의 경우 **분산**과 **기댓값**이 서로 같다는 사실을 알 수 있다.

> 이항분포

이항분포를 따르는 확률변수 $X \sim Bin(n, p)$에 대하여 $X = I_{1} + I_{2} + \cdots + I_{n}, \;\; I_{n} \stackrel{i.i.d} \sim Bern(p)$이다. 이제 양변을 **제곱**한다.

\begin{align}
X^{2} = (I_{1}^{2} + I_{2}^{2} + \cdots + I_{n}^{2}) + 2(I_{1}I_{2} + I_{1}I_{3} + \cdots + I_{n-1}I_{n}) \nonumber
\end{align}

\begin{align}
E(X^{2}) = n \cdot E(I_{1}^{2}) + \frac{2n(n-1)}{2} \cdot E(I_{1}I_{2}) = n \cdot E(I_{1}) + \frac{2n(n-1)}{2} \cdot E(I_{1}I_{2}) \nonumber
\end{align}

위 식에서 $E(I_{i}^{2}) = E(I_{i})$인 이유는 $I^{2}$의 값이 $I$의 값에 완전히 의존하기 때문이다. 즉, $I=0$이면 $I^{2}=0$이고 $I=1$이면 $I^{2}=1$이다. 따라서 $E(I_{i}^{2}) = p$이다. **LOTUS**를 이용하여 확인해 보아도 같은 결과가 나온다.

$I_{i}I_{j}$의 경우 두 확률변수의 값이 모두 $1$인 경우에만 $1$의 값을 가지고 나머지는 모두 $0$의 값을 가진다. 따라서 $E(I_{i}I_{j}) = p^{2}$이다.

이제 $X^{2}$의 **기댓값**과 $X$의 **분산**을 계산할 수 있다.

\begin{align}
E(X^{2}) = np + n(n-1)p^{2} = np(1-p+np) \nonumber
\end{align}

\begin{align}
Var(X) = E(X^{2}) - \\{E(X)\\}^{2} = np(1-p+np) - (np)^{2} = np(1-p) = npq \nonumber
\end{align}