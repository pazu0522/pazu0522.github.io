---
title:  "수리통계학(Stats 111) : Chapter 04. Unbiasedness, Consistency and Limiting Distribution (1)"
excerpt: "부산대학교 김충락 교수의 수리통계학 강의 정리"

categories:
  - 통계학
  - 수리통계학
tags:
  - 통계량
  - 확률벡터
  - 불편성
use_math: true
comments: true
last_modified_at: 2021-07-02T23:30:00
header:
  image: /assets/images/texture.jpg
---

<img src="/assets/images/stats_111/lesson_0401.jpg" width="80%" height="80%" title="dart" alt="dart"/> 

# 1. 기본 개념 정리

- [**통계량**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/stats111_0401/#%ED%86%B5%EA%B3%84%EB%9F%89)
- [**확률벡터**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/stats111_0401/#%ED%99%95%EB%A5%A0%EB%B2%A1%ED%84%B0)
- [**불편추정량**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/stats111_0401/#%EB%B6%88%ED%8E%B8%EC%B6%94%EC%A0%95%EB%9F%89)


## 통계량

- **모수(Parameter)**: **모집단(Population)**이 가지고 있는 다양한 특성. 평균과 분산이 대표적인 모수.
- **임의 표본(Random Sample; r.s.)**: $X_{i} \overset {i.i.d} \sim f_{X}(x)$일 때, 각각의 $X_{i}$를 임의 표본이라 함. 이하 **Random Sample** 또는 **r.s.**로 표기.
- **통계량(Statistic)**: Random Sample로 구성된 **함수** $T = \upsilon(X_{1}, \cdots, X_{n}) = \upsilon(\textbf{X}), \;\; \textbf{X} = (X_{1}, \cdots, X_{n})^{T}$를 통계량이라 함. 대표적인 통계량으로는 표본평균 $\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_{i}$과 표본분산 $s^{2} = \frac{1}{n-1} \sum_{i=1}^{n} (X_{i} - \bar{X})^{2}$이 있음.
- **추정량(Estimator)**: **모수**를 추정하기 위해 사용되는 **통계량**. 예를 들어, 표본평균은 모평균에 대한 추정량이고 표본분산은 모분산에 대한 추정량.


## 확률벡터

확률벡터 $\textbf{X} = (X_{1}, \cdots, X_{n})^{T}$과 확률벡터 $\textbf{Y} = (Y_{1}, \cdots, Y_{m})^{T}$일 때 확률벡터 $\textbf{X}$의 **기댓값**은 다음과 같이 정의된다.

\begin{align}
E(\textbf{X}) = \Big(E(X_{1}), \cdots, E(X_{n}) \Big)^{T} \nonumber
\end{align}

한편, 확률벡터 $\textbf{X}$의 **분산-공분산 행렬(Variance-Covariance Matrix)**은 다음과 같이 정의된다. 전체적인 형태는 확률변수의 분산 공식과 유사하다.

\begin{align}
Var(\textbf{X}) \nonumber
\end{align}

\begin{equation\*}
\begin{aligned}
&= E \Bigg\[ \Big(\textbf{X} - E(\textbf{X}) \Big) \Big(\textbf{X} - E(\textbf{X}) \Big)^{T} \Bigg\] \\\\\\ \\\\\\
&= E \Bigg\[ \Big(\textbf{X} - E(\textbf{X}) \Big) \Big(\textbf{X}^{T} - E(\textbf{X})^{T} \Big) \Bigg\]\\\\\\ \\\\\\
&= E \Big(\textbf{X} \textbf{X}^{T} - \textbf{X} \cdot E(\textbf{X})^{T} - E(\textbf{X}) \cdot \textbf{X}^{T} + E(\textbf{X}) \cdot E(\textbf{X})^{T} \Big) \\\\\\ \\\\\\
&= E(\textbf{X} \textbf{X}^{T}) - E(\textbf{X}) \cdot E(\textbf{X})^{T} - E(\textbf{X}) \cdot E(\textbf{X})^{T} + E(\textbf{X}) \cdot E(\textbf{X})^{T}  \\\\\\ \\\\\\
&= E(\textbf{X} \textbf{X}^{T}) - E(\textbf{X}) \cdot E(\textbf{X})^{T}
\end{aligned}
\end{equation\*}

분산-공분산 행렬의 구체적인 형태는 아래와 같이 **대각선 성분**은 확률벡터를 구성하는 각 확률변수 $X_{i}$의 **분산**이고 나머지 성분들은 서로 다른 두 확률변수의 **공분산**이다. $Cov(X_{i}, X_{j}) = Cov(X_{j}, X_{i})$이므로 $\sigma^{2}\_{ij} = \sigma^{2}\_{ji}$이다. 따라서 분산-공분산 행렬은 대각선 성분을 기준으로 **좌우대칭**의 형태이다.

\begin{align}
E \Bigg\[ \Big(\textbf{X} - E(\textbf{X}) \Big) \Big(\textbf{X} - E(\textbf{X}) \Big)^{T} \Bigg\] \nonumber
\end{align}

\begin{equation\*}
\begin{aligned}
&= E \begin{bmatrix}
    \begin{bmatrix}
      X_{1}-\mu_{1} \\\\\\
      X_{2}-\mu_{2} \\\\\\
      \vdots \\\\\\ 
      X_{n}-\mu_{n}
    \end{bmatrix} & 
    \begin{bmatrix}
      X_{1}-\mu_{1} & X_{2}-\mu_{2} &  \cdots & X_{n}-\mu_{n}
    \end{bmatrix}
  \end{bmatrix} \\\\\\ \\\\\\
&= E \begin{bmatrix}
    (X_{1}-\mu_{1})^{2} & (X_{1}-\mu_{1})(X_{2}-\mu_{2}) & \cdots & (X_{1}-\mu_{1})(X_{n}-\mu_{n}) \\\\\\ \\\\\\
    (X_{2}-\mu_{2})(X_{1}-\mu_{1}) & (X_{2}-\mu_{2})^{2} & \cdots & (X_{2}-\mu_{2})(X_{n}-\mu_{n})\\\\\\ \\\\\\
    \vdots & \vdots & \ddots  & \vdots \\\\\\ \\\\\\
    (X_{n}-\mu_{n})(X_{1}-\mu_{1}) & (X_{n}-\mu_{n})(X_{2}-\mu_{2}) & \cdots & (X_{n}-\mu_{n})^{2}
  \end{bmatrix} \\\\\\ \\\\\\
&= E \begin{bmatrix}
  \sigma^{2}\_{11} & \sigma^{2}\_{12} & \cdots & \sigma^{2}\_{1n} \\\\\\
  \sigma^{2}\_{21} & \sigma^{2}\_{22} & \cdots & \sigma^{2}\_{2n} \\\\\\
  \vdots & \vdots & \ddots  & \vdots \\\\\\
  \sigma^{2}\_{n1} & \sigma^{2}\_{n2} & \cdots & \sigma^{2}\_{nn}
  \end{bmatrix}
\end{aligned}
\end{equation\*}

확률벡터 하나를 추가하여, 확률벡터 $\textbf{X}$와 $\textbf{Y}$에 대한 **교차 분산-공분산 행렬(Cross Variance-Covariance Matrix)**은 다음과 같이 정의된다. 전체적인 형태는 두 확률변수의 공분산 공식과 유사하다.

\begin{align}
Cov(\textbf{X}, \textbf{Y}) = E \Bigg\[ \Big(\textbf{X} - E(\textbf{X}) \Big) \Big(\textbf{Y} - E(\textbf{Y}) \Big)^{T} \Bigg\] = E(\textbf{X} \textbf{Y}^{T}) - E(\textbf{X}) \cdot E(\textbf{Y})^{T} \nonumber
\end{align}

확률벡터 $\textbf{X}$가 $m$차원 열벡터이고 $\textbf{Y}^{T}$가 $n$차원 행벡터이므로 교차 분산-공분산 행렬의 크기는 $m \times n$이 될 것이다.

이제 행렬 $k \times m$ 크기의 행렬 $\textbf{A}$와 $p \times n$ 크기의 행렬 $\textbf{B}$를 정의한다. 이 때, 행렬과 확률벡터의 연산은 다음을 만족한다.

- $E(\textbf{A}\textbf{X}) = \textbf{A} \cdot E(\textbf{X})$
- $Cov(\textbf{A}\textbf{X}, \textbf{B}\textbf{Y}) = \textbf{A} \cdot  Cov(\textbf{X}, \textbf{Y}) \cdot  \textbf{B}^{T}$

두 번째 성질에 대한 전개 과정은 다음과 같다.

\begin{align}
Cov(\textbf{A}\textbf{X}, \textbf{B}\textbf{Y}) \nonumber
\end{align}

\begin{equation\*}
\begin{aligned}
&=E \Bigg\[ \Big(\textbf{A}\textbf{X} - E(\textbf{A}\textbf{X}) \Big) \Big(\textbf{B}\textbf{Y} - E(\textbf{B}\textbf{Y}) \Big)^{T} \Bigg\] \\\\\\ \\\\\\
&=E \Bigg\[ \Big(\textbf{A}\textbf{X} - E(\textbf{A}\textbf{X}) \Big) \Big(\textbf{Y}^{T} \textbf{B}^{T} - E(\textbf{B}\textbf{Y})^{T} \Big) \Bigg\] \\\\\\ \\\\\\
&=E \Big(\textbf{A}\textbf{X} \textbf{Y}^{T} \textbf{B}^{T} - \textbf{A}\textbf{X} \cdot E(\textbf{B}\textbf{Y})^{T} - E(\textbf{A}\textbf{X}) \cdot \textbf{Y}^{T}\textbf{B}^{T} - E(\textbf{A}\textbf{X}) \cdot E(\textbf{B}\textbf{Y})^{T} \Big) \\\\\\ \\\\\\
&= E(\textbf{A}\textbf{X} \textbf{Y}^{T}\textbf{B}^{T}) - E(\textbf{A}\textbf{X}) \cdot E(\textbf{B}\textbf{Y})^{T} - E(\textbf{A}\textbf{X}) \cdot E(\textbf{Y}^{T}\textbf{B}^{T}) + E(\textbf{A} \textbf{X}) \cdot E(\textbf{B}\textbf{Y})^{T} \\\\\\ \\\\\\
&= \textbf{A} \cdot E(\textbf{X} \textbf{Y}^{T}) \cdot \textbf{B}^{T} - \textbf{A} \cdot E(\textbf{X}) \cdot E(\textbf{Y}^{T}) \cdot \textbf{B}^{T} \\\\\\ \\\\\\
&= \textbf{A}  \Big( E(\textbf{X} \textbf{Y}^{T}) - E(\textbf{X}) \cdot E(\textbf{Y}^{T}) \Big)  \textbf{B}^{T} \\\\\\ \\\\\\
&= \textbf{A} \Big( E(\textbf{X} \textbf{Y}^{T}) - E(\textbf{X}) \cdot E(\textbf{Y})^{T} \Big) \textbf{B}^{T}
\\\\\\ \\\\\\
&= \textbf{A} \cdot Cov(\textbf{X}, \textbf{Y}) \cdot \textbf{B}^{T}
\end{aligned}
\end{equation\*}


## 불편추정량

$X_{i} \overset {i.i.d} \sim f_{X}(x: \boldsymbol{\theta}), \;\; \boldsymbol{\theta} \in \Omega$을 가정한다. 여기서 $\boldsymbol{\theta}$는 확률분포의 모수 벡터를 나타낸다. 예를 들어, 정규분포라면 $\boldsymbol{\theta} = (\mu, \sigma^{2})$가 될 것이다. 또한, 모수 벡터 $\boldsymbol{\theta}$는 **모수공간(Parameter Space)** $\Omega$의 원소이다.

이 때, 어떤 **추정량** $T = \upsilon (X_{1}, \cdots, X_{n})$이 $\boldsymbol{\theta}$에 대하여 $E(T) = \boldsymbol{\theta}$가 성립한다면 $T$는 $\boldsymbol{\theta}$에 대한 **불편추정량(Unbiased Estimator; u.e.)**이다.

다시 말해 $E(T) - \boldsymbol{\theta} = Bias(T)$일 때, $Bias(T) = \textbf{O}$이면 $T$는 불편추정량이다.