---
title:  "확률론 기초(Stats 110) : Lesson 33. Markov Chain (3)"
excerpt: "하버드 대학교 조셉 K. 블리츠타인 교수의 Stats 110 33강 정리"

categories:
  - 통계학
  - 확률론
tags:
  - 마르코프 체인
use_math: true
comments: true
last_modified_at: 2021-06-25T23:30:00
header:
  image: /assets/images/texture.jpg
---

<img src="/assets/images/stats_110/lesson_33.jpg" width="80%" height="80%" title="google" alt="google"/> 

# 1. 마르코프 체인

- [**가역적 마르코프 체인 일반화**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%ED%99%95%EB%A5%A0%EB%A1%A0/stats110_33/#%EA%B0%80%EC%97%AD%EC%A0%81-%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84-%EC%B2%B4%EC%9D%B8-%EC%9D%BC%EB%B0%98%ED%99%94)
- [**페이지 랭크**](https://pazu0522.github.io/%ED%86%B5%EA%B3%84%ED%95%99/%ED%99%95%EB%A5%A0%EB%A1%A0/stats110_33/#%ED%8E%98%EC%9D%B4%EC%A7%80-%EB%9E%AD%ED%81%AC)


## 가역적 마르코프 체인 일반화

<img src="/assets/images/stats_110/lesson_33_1.jpg" width="40%" height="40%" title="example1" alt="example1"/>

현재 상태에서 다음 상태로 이동할 때의 확률이 모두 동일하지 않은 경우 즉, **가중치**가 존재하는 경우의 **임의 보행**(Random Walk) 네트워크를 고려한다. 모든 가중치 $w_{ij}$는 $0$보다 크거나 같은 값을 가진다. 가중치가 $0$이 되는 경우는 상태 $i$와 상태 $j$간 직접 이동이 불가능한 경우에 해당한다. 또한, $w_{ij} = w_{ji}$가 성립한다고 가정한다.

위와 같은 형태의 마르코프 체인에서 다음 단계에 상태 $i$에서 상태 $j$로 이동할 확률은 가중치 $w_{ij}$에 비례한다. 만약 상태 $i$에서 상태 $j$로 직접 이동이 가능한 경우, 확률이 가중치에 비례하기 때문에 $q_{ij} = \frac{w_{ij}}{\sum_{k}w_{ik}}$가 성립한다. 그러면 다음과 같은 전개가 가능하다.

\begin{align}
\sum_{k}w_{ik} \cdot q_{ij} = w_{ij} = w_{ji} = \sum_{k}w_{jk} \cdot q_{ji} \nonumber
\end{align}

위 전개는 결국 $s_{i} \cdot q_{ij} = s_{j} \cdot q_{ji}$의 형태와 같다. 따라서 해당 임의 보행 네트워크는 **정상 확률분포**가 존재하는 **가역적 마르코프 체인**으로 쉽게 변환할 수 있다. 정상 확률분포에서 상태 $i$에 대한 확률을 $s_{i}$라 하면 $s_{i}$는 $\sum_{k}w_{ik}$에 비례한다. 가중치의 합계가 $1$이라는 가정을 하지 않았으므로 $s_{i}$는 $\sum_{k}w_{ik}$를 전체 가중치의 합으로 나눈 결과와 같다. 그렇다면 정상 확률분포를 따르는 확률벡터 $\overset {\rightarrow} s$는 다음과 같다.

\begin{align}
\overset {\rightarrow} s = \frac{1}{\sum_{l}^{j}\sum_{k}^{i}w_{lk}} \left( \sum_{k}w_{1k}, \sum_{k}w_{2k}, \cdots, \sum_{k}w_{ik} \right) \nonumber
\end{align}

가중치가 존재하는 임의 보행 네트워크를 가역적 마르코프 체인으로 변환 가능한 것처럼 반대 방향으로의 변환도 항상 가능하다. 가역적 마르코프 체인을 보유한 상태라면 $s_{i}$와 $q_{ij}$의 값을 모두 알고 있는 상황일 것이다. 이러한 상황에서 마르코프 체인을 가중치가 존재하는 임의 보행 네트워크로 변환하기 위해서는 결국 가중치의 값을 알아내야 한다.

만약 가중치 $w_{ij}$가 $s_{i} \cdot q_{ij}$와 같다고 두면, 마르코프 체인이 가역적이기 때문에 $w_{ij} = s_{i} \cdot q_{ij} = s_{j} \cdot q_{ji} = w_{ji}$가 성립할 것이다. 가중치 $w_{ij}$를 위와 같이 설정하였을 때 상태 $i$에서 상태 $j$로의 전이 확률 $q_{ij}$가 그대로 유지된다면 설정한 가중치는 합당한 가중치가 될 것이다. 이제 설정한 가중치를 기준으로 **전이 확률** $P(X_{n+1} = j \mid X_{n} =i)$을 계산한다.

\begin{align}
P(X_{n+1} = j \mid X_{n} =i) = \frac{w_{ij}}{\sum_{k}w_{ik}} = \frac{s_{i} \cdot q_{ij}}{\sum_{k}s_{i} \cdot q_{ik}} = \frac{s_{i} \cdot q_{ij}}{s_{i} \cdot \sum_{k}q_{ik}} = q_{ij}, \quad \because \sum_{k}q_{ik} = 1 \nonumber
\end{align}

위 전개는 $s_{i} \neq 0$일 때 성립한다. 만약 $s_{i} = 0$이라면 상태 $i$는 아무런 의미를 가지지 않기 때문에 임의 보행 네트워크와 마르코프 체인에서 제외하면 그만이다. 가중치 $w_{ij}$를 $s_{i} \cdot q_{ij}$와 같도록 설정했을 때 합당한 전이 확률 $q_{ij}$를 가지는 것을 확인하였다. 따라서 가역적 마르코프 체인은 위와 같은 방법으로 가중치를 설정하여 가중치가 존재하는 임의 보행 네트워크로 항상 변환할 수 있다.


## 페이지 랭크

대표적인 **비가역적 마르코프 체인**으로 **구글**의 **페이지 랭크**(Page Rank) 알고리즘이 있다. 세상에 존재하는 수많은 웹 페이지의 중요도를 어떻게 결정할 것인가? 페이지 랭크 알고리즘에 의하면, 각 페이지의 중요도는 해당 페이지에 연결된 링크의 개수뿐만 아니라 페이지에 연결된 페이지들의 중요도에 의해 결정된다.

<img src="/assets/images/stats_110/lesson_33_2.jpg" width="40%" height="40%" title="example2" alt="example2"/>

위 그림은 웹 페이지 간 연결 관계를 도식화하여 보여주고 있다. 원의 크기를 페이지 랭크 알고리즘 상에서의 중요도가 아닌, 일반적 의미에서 각 페이지의 중요도라 하면 파란색과 주황색 페이지는 같은 중요도를 가지는 것처럼 보인다. 파란색과 주황색 페이지에 연결된 링크의 개수도 각각 네 개씩으로 동일하다. 그러나 페이지 랭크 알고리즘은 파란색 페이지에 더 높은 중요도를 부여할 것이다. 왜냐 하면 파란색 페이지에 연결된 초록색 페이지들의 중요도가 주황색 페이지에 연결된 노란색 페이지들의 중요도보다 높기 때문이다.

$s_{j}$를 각 페이지의 **중요도**를 반영한 일종의 점수로 정의한다. 그렇다면 $s_{j} = \sum_{i}s_{i} q_{ij}$로 쓸 수 있다. 어떤 페이지 $P$에 페이지 $A$와 페이지 $B$가 연결되어 있는 상황을 가정한다. 만약 페이지 $A$에서는 다른 $100$개의 페이지로 이동할 수 있고 페이지 $B$에서는 다른 $10$개의 페이지로 이동할 수 있다면 페이지 $B$에서 페이지 $P$로 링크의 중요도는 페이지 $A$에서 페이지 $P$로 링크의 중요도보다 높다. 웹 서핑을 하는 사용자가 무작위로 링크를 선택한다고 가정하였을 때, 페이지 $A$에서 페이지 $P$로 이동할 확률은 $\frac{1}{100}$이지만 페이지 $B$에서 페이지 $P$로 이동할 확률은 $\frac{1}{10}$이기 때문이다. 이렇게 특정 페이지가 선택될 확률을 반영하는 것이 $q_{ij}$이다.

결국 $s_{j} = \sum_{i} s_{i} q_{ij}$는 어떤 페이지의 점수는 페이지에 연결된 다른 페이지의 중요도에 다른 페이지에서 해당 페이지로 넘어올 확률을 곱한 값의 합계라는 사실을 의미한다.

<img src="/assets/images/stats_110/lesson_33_3.jpg" width="40%" height="40%" title="example3" alt="example3"/>

만약 페이지가 위와 같은 형태로 연결되어 있다면 **전이 행렬** $Q$는 다음과 같다.

\begin{align}
Q = (q_{ij}) =
  \begin{pmatrix}
  0 & 1/2 & 1/2 & 0 \\\\\\ 
  1/2 & 0 & 1/2 & 0 \\\\\\ 
  0 & 0 & 0 & 1 \\\\\\ 
  1/4 & 1/4 & 1/4 & 1/4
  \end{pmatrix}
, \quad i, j \in \\{1, 2, 3, 4\\} 
\nonumber
\end{align}

$4$번 페이지는 링크가 없어서 원래대로라면 전이 행렬의 마지막 행은 $(0, 0, 0, 0)$이 되어야 하지만 페이지 랭크 알고리즘은 이러한 경우 웹 서핑을 하는 사용자가 오히려 다른 어떤 페이지로도 갈 수 있다고 가정하여 전이 확률을 고르게 분배한다. 이제 각 페이지의 중요도를 대변하는, $\overset {\rightarrow} s = \overset {\rightarrow} sQ$를 만족하는 확률 벡터 $\overset {\rightarrow} s$를 찾아야 한다. 물론 **정상 확률분포**를 따르는 확률 벡터 $\overset {\rightarrow} s$는 성분의 합이 $1$이 되도록 **정규화**된 벡터이다.

그러나 실제 웹 페이지의 개수는 굉장히 많기 때문에 **가우스 소거법**을 사용한다고 해도 $\overset {\rightarrow} s$를 계산하는 일은 현실적으로 불가능한 일이다. 구글의 페이지 랭크 알고리즘은 전이 행렬을 다음과 같이 변형하여 계산 문제를 해결하였다.

\begin{align}
G = \alpha Q + \frac{1-\alpha}{m} J, \quad 0 < \alpha < 1 \nonumber
\end{align}

여기서 $m$은 페이지의 개수를 의미한다. $J$는 모든 성분이 $1$인 $m \times m $ 행렬이다. 구글은 웹 서핑을 하는 사용자가 $\alpha$의 확률로 링크롤 통해 다른 페이지로 이동한다고 가정하였으며 $(1-a)$의 확률로 다른 마르코프 체인으로 순간이동 즉, 현재 페이지에서 링크를 따라가지 않고 새로운 위치에서 웹 서핑을 재개한다고 가정하였다. 즉, $\alpha$는 사용자의 실제 웹 서핑 행태를 반영한 계수인데 초기 구글은 $\alpha$의 값을 $0.85$로 설정하였다고 전해진다.

마르코프 체인 상에서 순간이동이 가능하기 때문에 현재 페이지에서 다른 어떤 페이지로도 이동이 가능하게 되었다. 따라서 위와 같이 변형된 마르코프 체인은 **기약성**이 보장된다. 또한, 페이지의 개수는 그 수가 매우 많지만 여전히 **유한**하다. 따라서 **정상 확률분포**가 반드시 존재한다.

시간 $0$에서 임의로 설정한 확률 벡터 $\overset {\rightarrow} t$에 대하여, $n \rightarrow \infty$에 따라 $\overset {\rightarrow} tG^{n} \rightarrow \overset {\rightarrow} s$가 성립한다. $n=1$일 때 $\overset {\rightarrow} t G = \alpha \cdot \overset {\rightarrow} t Q + \frac{1-\alpha}{m} \cdot \overset {\rightarrow} t J$이다. $m$은 페이지의 개수이므로 매우 큰 값이다. 따라서 $Q$는 $m \times m$의 거대한 행렬이 된다. 그런데 한 페이지에서 다른 페이지로의 링크의 개수는 $m$에 비하면 턱없이 적기 때문이 행렬 $Q$를 구성하는 대부분의 성분은 $0$이 될 것이다. 따라서 $n$이 커지더라도 행렬의 곱셈 연산이 간소해진다.

뿐만 아니라, $\overset {\rightarrow} t J$에서 $J$의 모든 성분이 $1$이고 $\overset {\rightarrow} t$의 성분의 총합 또한 $1$이기 때문에 $\overset {\rightarrow} t J$의 계산 결과는 $(1, 1, \cdots, 1)$의 $m$차원 벡터가 된다. 따라서 두 번째 항의 계산 결과도 매우 빠르게 얻을 수 있다.

$tG^{k}$을 매우 빠른 속도로 계산할 수 있기 때문에 연산을 무한히 반복하면 정상 확률분포를 얻을 수 있다. 현실적으로 무한 연산은 불가능하지만 충분한 횟수의 연산을 거치면 정상 확률분포를 따르는 확률 벡터 $\overset {\rightarrow} s$의 근사치를 얻어낼 수 있게 된다.
